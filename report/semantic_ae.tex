\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

\title{Image compression with autoencoders using semantic loss function}

\author{Romain Meynard\\
{\tt\small romain.meynard@student.ecp.fr}
\and
Wenceslas des Déserts \\
{\tt\small venceslas.danguy-des-deserts@student.ecp.fr}
\and
Silvestre Perret \\
{\tt\small silvestre.perret@student.ecp.fr}	 \\
\\
CentraleSupélec\\
}

\maketitle
%\thispagestyle{empty}

\begin{abstract}
Lossy image compression is the task of finding a representation of an image that keeps most of its content while significantly reducing its storage size. This task allows storage and transfer of images for a minimum cost and is commonly achieved using algorithms such as JPEG 2000. Recent studies in image compression using a deep learning approach make use of either autoencoders with pixelwise loss or Generative Adversarial Networks (GANs) using semantic and texture losses. The aim of this study is to apply semantic and texture losses to autoencoders. 
\end{abstract}

\section{Introduction}

\section{State of the art}
We should write a few lines about \cite{theis_lossy_2017} (which model we used), \cite{sajjadi_enhancenet:_2016} (gave us ideas for the loss functions) and also about \cite{gatys_texture_2015} in which texture loss was first introduced.

\section{Model}

Explain our archictecture and all the small tricks we used (custom derivatives, all this stuff). 

\section{Experiments}

Which dataset did we use, why and how ?

\section{Results}

Probably a bunch of nice images showing how great our autoencoder performs.

\section{Discussion} % Conclusion ?

What did we do ? Are our results good ? What could we do next if we were not writing this report less than four days before the deadline ?

{\small
\bibliographystyle{ieee}
\bibliography{semantic_ae}
}

\end{document}
