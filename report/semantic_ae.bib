
@article{theis_lossy_2017,
	title = {Lossy {Image} {Compression} with {Compressive} {Autoencoders}},
	url = {http://arxiv.org/abs/1703.00395},
	abstract = {We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.},
	urldate = {2018-04-11},
	journal = {arXiv:1703.00395 [cs, stat]},
	author = {Theis, Lucas and Shi, Wenzhe and Cunningham, Andrew and Huszár, Ferenc},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.00395},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {arXiv\:1703.00395 PDF:/home/alsyia/Zotero/storage/U7J7MW83/Theis et al. - 2017 - Lossy Image Compression with Compressive Autoencod.pdf:application/pdf;arXiv.org Snapshot:/home/alsyia/Zotero/storage/9AM9CBQR/1703.html:text/html}
}

@article{sajjadi_enhancenet:_2016,
	title = {{EnhanceNet}: {Single} {Image} {Super}-{Resolution} {Through} {Automated} {Texture} {Synthesis}},
	shorttitle = {{EnhanceNet}},
	url = {http://arxiv.org/abs/1612.07919},
	abstract = {Single image super-resolution is the task of inferring a high-resolution image from a single low-resolution input. Traditionally, the performance of algorithms for this task is measured using pixel-wise reconstruction measures such as peak signal-to-noise ratio (PSNR) which have been shown to correlate poorly with the human perception of image quality. As a result, algorithms minimizing these metrics tend to produce over-smoothed images that lack high-frequency textures and do not look natural despite yielding high PSNR values. We propose a novel application of automated texture synthesis in combination with a perceptual loss focusing on creating realistic textures rather than optimizing for a pixel-accurate reproduction of ground truth images during training. By using feed-forward fully convolutional neural networks in an adversarial training setting, we achieve a significant boost in image quality at high magnification ratios. Extensive experiments on a number of datasets show the effectiveness of our approach, yielding state-of-the-art results in both quantitative and qualitative benchmarks.},
	urldate = {2018-04-11},
	journal = {arXiv:1612.07919 [cs, stat]},
	author = {Sajjadi, Mehdi S. M. and Schölkopf, Bernhard and Hirsch, Michael},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.07919},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: main paper and supplementary material},
	file = {arXiv\:1612.07919 PDF:/home/alsyia/Zotero/storage/MIW7Z4DB/Sajjadi et al. - 2016 - EnhanceNet Single Image Super-Resolution Through .pdf:application/pdf;arXiv.org Snapshot:/home/alsyia/Zotero/storage/B5XHQJFG/1612.html:text/html}
}

@article{gatys_texture_2015,
	title = {Texture {Synthesis} {Using} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.07376},
	abstract = {Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.},
	urldate = {2018-04-11},
	journal = {arXiv:1505.07376 [cs, q-bio]},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	month = may,
	year = {2015},
	note = {arXiv: 1505.07376},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: Revision for NIPS 2015 Camera Ready. In line with reviewer's comments we now focus on the texture model and texture synthesis performance. We limit the relationship of our texture model to the ventral stream and its potential use for neuroscience to the discussion of the paper},
	file = {arXiv\:1505.07376 PDF:/home/alsyia/Zotero/storage/MBJ5JHR8/Gatys et al. - 2015 - Texture Synthesis Using Convolutional Neural Netwo.pdf:application/pdf;arXiv.org Snapshot:/home/alsyia/Zotero/storage/CVGAWTVX/1505.html:text/html}
}